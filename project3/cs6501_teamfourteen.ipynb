{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "930vlW5BrOtq"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1vK33e_EqaHgBHcbRV_m38hx6IkG0blK_\" width=\"350\"/>\n",
        "</div> \n",
        "\n",
        "#**Artificial Intelligence - MSc**\n",
        "##CS6501 - MACHINE LEARNING APPLICATIONS \n",
        "\n",
        "###Instructor: Enrique Naredo\n",
        "###CS6501_Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "LqXD_IwUQuBF"
      },
      "outputs": [],
      "source": [
        "#@title Current Date\n",
        "Today = '2021-11-28' #@param {type:\"date\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uzDKau31OjVO"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "#@markdown ### Enter your details here:\n",
        "Team_Number = \"14\" #@param {type:\"string\"}\n",
        "Team_Name = \"TeamFourteen\" #@param {type:\"string\"}\n",
        "Student_ID = \"21183147\" #@param {type:\"string\"}\n",
        "Student_full_name = \"Sarthak Punjabi\" #@param {type:\"string\"}\n",
        "Student_ID = \"21006415\" #@param {type:\"string\"}\n",
        "Student_full_name = \"Shagil chaudhary\" #@param {type:\"string\"}\n",
        "Student_ID = \"21041784\" #@param {type:\"string\"}\n",
        "Student_full_name = \"humraj singh sorout\" #@param {type:\"string\"}\n",
        "Student_ID = \"21143838\" #@param {type:\"string\"}\n",
        "Student_full_name = \"Anupriya Shanmugam\" #@param {type:\"string\"}\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r39xGZckTpKx"
      },
      "outputs": [],
      "source": [
        "#@title Notebook information\n",
        "Notebook_type = 'Assignment' #@param [\"Example\", \"Lab\", \"Practice\", \"Etivity\", \"Assignment\", \"Exam\"]\n",
        "Version = \"Final\" #@param [\"Draft\", \"Final\"] {type:\"raw\"}\n",
        "Submission = True #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A0Z6S-r6DpA"
      },
      "source": [
        "# INTRODUCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXvuobUCALr1"
      },
      "source": [
        "The aim of this experiment is to design and develop a system to predict the price of a house based on features given in the dataset. The predictions from this system will simultaneously be submitted to a ranked Kaggle competition in addition to the usual submission on SULIS. We are expected to implement the machine learning concepts studied during this week, i.e. Natural Language Processing and Long Short Term Memory. We are also presented with the opportunity to apply prior knowledge to design an effective model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg7VCbX77eAA"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_3LlZvx7AQgB"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/rd/5k6577r15nx8fs98hy3sr4940000gn/T/ipykernel_1609/4091661565.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfolium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import hstack \n",
        "import folium\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from folium.plugins import MarkerCluster\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import\tmean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Roko8uRJAQc9"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "df = pd.read_csv(\"HousePrice_Train.csv\")\n",
        "sns.set_theme(style=\"whitegrid\", \n",
        "              palette=\"colorblind\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23_-NsbA6Ixz"
      },
      "outputs": [],
      "source": [
        "# View data\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI9V9IcCldxB"
      },
      "source": [
        "the description of data shows that it has a mix of textual, numerical, and categorical data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35dBCwnwi251"
      },
      "source": [
        "## Exploring the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj0g60bbkSYc"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0rMUaVtkQp1"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8y92X_gmp18"
      },
      "source": [
        "The describe() function gives us the statistical properties of the data. It gives us stats about numerical data only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HK0jhbRBkUK0"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNI-aYBci3WE"
      },
      "outputs": [],
      "source": [
        "df.head(5) # First five rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVFeUIU2nLzb"
      },
      "source": [
        "Here, we look for the null values present in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO2gvvZ_kbJ0"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum() # Check number of missing values per column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frTxqe7XnMpq"
      },
      "source": [
        "Here, BER_class and Services columns have several missing values that has to be dealt with.\n",
        "We plot histogram for few specific columns to better understand the data by visualisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bix0L6-KCXmY"
      },
      "outputs": [],
      "source": [
        "# Draw histograms for data\n",
        "column_list = ['Num_Bathrooms', 'Num_Beds', 'BER_class', 'Type','Surface', 'Price'] #specified column list\n",
        "for column in column_list: \n",
        "    plt.figure(figsize=(20,5)) \n",
        "    sns.histplot(df[column]) # Create histogram\n",
        "    plt.show() # Show histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddEIG9iBprKx"
      },
      "source": [
        "Scatter plots are used for the examination of the relationship between the predictor variable and the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwBq8aa02SxI"
      },
      "outputs": [],
      "source": [
        "# scatterplot between numerical variables and price\n",
        "column_list = ['Num_Bathrooms', 'Num_Beds', 'Latitude', 'Longitude','Surface']\n",
        "for column in column_list:\n",
        "    plt.figure(figsize=(20,5)) \n",
        "    sns.scatterplot(df[column],df[\"Price\"])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uop5j2rastET"
      },
      "source": [
        "following is observed through the scatterplots:\n",
        "* There is a slight correlation between number of bathrooms and price\n",
        "* The same can be said for number of bedrooms.\n",
        "* There's a very slight correlation between location and price \n",
        "* There is a strong positive correlation between price and surface area. Not surprising.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-7qtp3uqdkl"
      },
      "outputs": [],
      "source": [
        "# Box plots are used for the purpose of detecting the outliers.\n",
        "column_list = ['BER_class','Type']\n",
        "for column in column_list:\n",
        "    plt.figure(figsize=(20,5)) \n",
        "    sns.boxplot(df[column],df[\"Price\"])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_9xFQ3UtfRT"
      },
      "source": [
        "* The box plots for BER class showcases quite a few outliers.\n",
        "* There are plenty of outliers for the “detached” home too.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yOS0juGtpSz"
      },
      "source": [
        "We calculate a correlation matrix and plot it using a heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyV5MLypke40"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize= (10, 10)) # Define plot size, increase if the graph is crowded\n",
        "sns.heatmap(df.iloc[:,2:].corr(),square=True, annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0QyIjMZuGtN"
      },
      "source": [
        "The correlation heatmap shows that price is highly correlated to the number of bedrooms and bathrooms. This could be caused due to outliers. This is why we observe the relationship between variables using scatter plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3sP_VAzua3O"
      },
      "source": [
        "The “folium” package allows us to visualize a set of coordinates on an interactive map on interactive python notebooks, so we use it here to draw up a map of our houses. This should also help us investigate the outlier we observed earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNiNrEGsFzUp"
      },
      "outputs": [],
      "source": [
        "locations = df[['Latitude', 'Longitude']] \n",
        "locationlist = locations.values.tolist() # converting to a list\n",
        "\n",
        "# Set map parameters.\n",
        "map = folium.Map(location=[df['Latitude'].median(), df['Longitude'].median()], # Center the default map location around our data\n",
        "                 zoom_start=12) \n",
        "marker_cluster = MarkerCluster().add_to(map) \n",
        "for point in range(0, len(locationlist)): # Iterate through list of coordinates\n",
        "    folium.Marker(locationlist[point], popup=df['Location'][point]).add_to(marker_cluster) \n",
        "map # Display the map. Takes a while to load!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zbwArcEvlds"
      },
      "source": [
        "Most of the data is in and around Dublin. However, a singular point in the United Kingdom. Considering we’re looking at Ireland housing data, it’s safe to say this point was included in our dataset accidently. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nLkCiYJ9Zan"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcujSEA739V2"
      },
      "source": [
        "### Removing outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHlaHXZovyLr"
      },
      "source": [
        "The first thing we do is remove the outlier we just spotted in the data exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiQylVfS16y4"
      },
      "outputs": [],
      "source": [
        "df[df[\"Latitude\"] < 52.6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbxm61R92LhB"
      },
      "outputs": [],
      "source": [
        "df[df[\"Longitude\"] > -2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CFUlzDJ5RAI"
      },
      "outputs": [],
      "source": [
        "outlier_index = df[df[\"ID\"] == 12270559].index # Store index of row where ID = 12270559\n",
        "df.drop(outlier_index, inplace=True) # Delete it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LWH4HTQwOTM"
      },
      "source": [
        "We can verify that this point is gone by searching for the same ID again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5icBGhm3KqR"
      },
      "outputs": [],
      "source": [
        "df[df[\"ID\"] == 12270559] # find rows where ID = 12270559\n",
        "df[df[\"Num_Bathrooms\"] > 10]\n",
        "df[df[\"Num_Beds\"] > 10]\n",
        "fancy_house_IDs = [\"12381836\",\"11780612\",\"12085770\"]\n",
        "for houseID in fancy_house_IDs:\n",
        "    outlier_index = df[df[\"ID\"] == houseID].index\n",
        "    df.drop(outlier_index, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwjJtZHP33em"
      },
      "source": [
        "We can also directly check the condition and drop the outliers by index as such:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTVyMEIc3wHb"
      },
      "outputs": [],
      "source": [
        "outlier_index = df[df[\"Surface\"] > 5000].index \n",
        "df.drop(outlier_index, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX1EWX3z4Ncx"
      },
      "source": [
        "The above code gets rid of 2 houses with an oddly high surface area."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T902atji4S3w"
      },
      "source": [
        "We can validate our oulier removal by running the plots again. Let's check the heatmap again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_MKBn2B4ZsJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize= (10, 10))\n",
        "sns.heatmap(df.iloc[:,2:].corr(),square=True, annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYeUy3h6Mdjh"
      },
      "source": [
        "### Numerical and categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPboaEnKxBpb"
      },
      "source": [
        "Initially we use a subset of the original dataframe containing only numerical and categorical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7viNW9_9cU3"
      },
      "outputs": [],
      "source": [
        "df_numeric = df[[\"Location\",\"Num_Bathrooms\",\"Num_Beds\",\"BER_class\",\"Latitude\",\"Longitude\",\"Type\",\"Surface\",\"Price\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH7D7fPMzzCL"
      },
      "source": [
        "In order to apply machine learning algorithms to categorical data, we must transform it using the naive approach known as “encoding”, specifically “one-hot encoding”.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTySCc60Qp5S"
      },
      "outputs": [],
      "source": [
        "# Initialize (set up) encoder to do the heavy lifting for us\n",
        "encoder = OneHotEncoder(drop=\"first\", # Remove the first column\n",
        "                        sparse=False, # Set to return an array instead of a matrix\n",
        "                        handle_unknown=\"ignore\") # Don't throw errors up if null values are found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPIZJoOWg-Vu"
      },
      "outputs": [],
      "source": [
        "columns_to_replace = [\"Location\",\"BER_class\",\"Type\"] # List out columns to encode \n",
        "encoded_data = encoder.fit_transform(df[columns_to_replace]) # Encode them\n",
        "encoded_data = pd.DataFrame(encoded_data) # Convert this to a dataframe\n",
        "encoded_data.head() # View data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HSYrj_XWiav"
      },
      "outputs": [],
      "source": [
        "encoded_data.columns = encoder.get_feature_names_out() # Rename encoded columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wrgTJmHXCaJ"
      },
      "outputs": [],
      "source": [
        "encoded_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwSTWX0uXNNf"
      },
      "source": [
        "Here, we only get the encoded data after this the next step is to drop the original columns and join the encoded data with the actual data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI-u7dGRVpuw"
      },
      "outputs": [],
      "source": [
        "for column in columns_to_replace: # Iterate through list of columns\n",
        "    df_numeric.drop(column ,axis=1, inplace=True) # Drop (delete) the ones we encoded\n",
        "df_numeric = encoded_data.join(df_numeric) # Concatenate (join) the dataframes\n",
        "df_numeric.columns.tolist() # Check column names of final encoded data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK0a6UiQ4MHU"
      },
      "source": [
        "After getting all the data together the next step is to scale the data to make the model computationally efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6NsafWU2DI3"
      },
      "outputs": [],
      "source": [
        "df_numeric_noscale = df_numeric.copy()\n",
        "scaler = MinMaxScaler() # Initialize scaler \n",
        "for column in df_numeric.columns:\n",
        "    df_numeric[column]=pd.DataFrame(scaler.fit_transform(df_numeric[column].values.reshape(-1,1))) # Apply scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnX20qD248om"
      },
      "source": [
        "We can, observe the results of scaling by using the describe() function again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNIBdJ3C-0Ki"
      },
      "outputs": [],
      "source": [
        "df_numeric.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgKSwPJwMcAa"
      },
      "source": [
        "### Textual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92jOncTcEjSr"
      },
      "outputs": [],
      "source": [
        "df_textual = df[[\"Description\",\"Services\",\"Features\",\"Price\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT1V4w3P7usR"
      },
      "source": [
        "We transform words into numbers to feed them into our traditional machine learning algorithms. A major difference here is that we initially have to clean the data and extract the most significant words, before assigning values to them, a process known as “vectorizing”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kxMEPwZ9iQ1"
      },
      "source": [
        "To start off, we initialize our lemmatizer object.\n",
        "\n",
        "We also download a few sets:\n",
        "* Wordnet: the Lemmatization dataset.\n",
        "* Stopwords: Common “stop” words that do not add much to the context of a sentence.\n",
        "* Punkt: Punctuation marks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SowqHdq0GoOw"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer() # Initialize Lemmatizer\n",
        "nltk.download('wordnet')# Download lemmatization database\n",
        "nltk.download('stopwords') # Download list of stop words\n",
        "nltk.download('punkt') # download list of punctuation symbols\n",
        "stopwords_set = set(stopwords.words('english')) # Define list of stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1vTklCj9mZO"
      },
      "source": [
        "Shows us the list of stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs5iAqld8DDF"
      },
      "outputs": [],
      "source": [
        "stopwords_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlLSPkdt-T14"
      },
      "source": [
        "We now define a cleaning function that should be able to reduce entire sentences down to their base forms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o5zo6_RRxnR"
      },
      "outputs": [],
      "source": [
        "def clean(row):\n",
        "    input_data = str(row)\n",
        "\n",
        "    lowertext = input_data.lower() # convert to lower case\n",
        "    tokens = word_tokenize(lowertext) # Tokenize\n",
        "    df_stopwords=[word for word in tokens if word not in stopwords_set] # Remove stopwords\n",
        "    df_punctuation=[re.sub(r'['+string.punctuation+']+', ' ', i) for i in df_stopwords] # Remove Punctuation and split 's, 't, 've with a space for filter\n",
        "    df_whitespace = ' '.join(df_punctuation) # Remove multiple whitespace\n",
        "    lemmatizer = WordNetLemmatizer() # Initialize lemmatizer\n",
        "    df_lemma = lemmatizer.lemmatize(df_whitespace) # Lemmatize\n",
        "    df_lemma_tokenized = word_tokenize(df_lemma) # Tokenize again\n",
        "    df_lemma_shortwords = [re.sub(r'^\\w\\w?$', '', i) for i in df_lemma_tokenized] # Remove short words\n",
        "    df_lemma_whitespace =' '.join(df_lemma_shortwords) # Join whitespace again\n",
        "    df_lemma_multiplewhitespace = re.sub(r'\\s\\s+', ' ', df_lemma_whitespace) # Join multiple white spaces\n",
        "    df_clean = df_lemma_multiplewhitespace.lstrip(' ') #Remove any whitespace at the front of the sentence\n",
        "\n",
        "    return df_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRPZ_YsZ_nvG"
      },
      "source": [
        "Taking it step by step:\n",
        "* Lowercase: easier to process everything when you don’t have to worry about case sensitivity.\n",
        "* Tokenize: Split sentences into words, with each word being a “token”.\n",
        "* Remove stopwords: Scans through the tokens of each sentence, checks them against the big list of stopwords, and only keeps them if the aren’t present in the stopwords list.\n",
        "* Remove punctuation: Punctuation usually doesn’t add much to the context of a sentence, and can be very ambiguous depending on usage. It’s better to remove them altogether.\n",
        "* Whitespace: A lot of these operations don’t actually remove the offending characters, but rather replace them with a space. We join multiple spaces together quite often here.\n",
        "* Lemmatization: Reduces words to their base form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpq31es0ULja"
      },
      "outputs": [],
      "source": [
        "df_textual[\"Description\"][0] # Original\n",
        "clean(df_textual[\"Description\"][0]) # Cleaned\n",
        "df_textual_cleaned = df_textual.iloc[:,:-1].applymap(clean) # Apply cleaning to entire dataset\n",
        "df_textual_cleaned = pd.concat([df_textual_cleaned,df_textual[\"Price\"]],axis=1) # concatenate price\n",
        "df_textual_cleaned.head() # View cleaned dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iodIb1JIBOk1"
      },
      "source": [
        "The next step is to vectorize the data and apply to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL91ShB19RPw"
      },
      "source": [
        "# NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYmCp6dBpLgB"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy6hG20Fkq-B"
      },
      "source": [
        "The TF-IDF for each term (word) is calculated by \n",
        "\n",
        "$ {tf} * {idf}$\n",
        "\n",
        "Where\n",
        "\n",
        "${tf} (t,d)={\\frac {f_{t,d}}{\\sum _{t'\\in d}{f_{t',d}}}}$\n",
        "\n",
        "${idf} (t,D)=\\log {\\frac {N}{|\\{d\\in D:t\\in d\\}|}}$\n",
        "\n",
        "Complex formulae aside, the term frequency(TF)  is simply the number of times a term(word) occurs in a document(sentence). Inverse Document Frequency(IDF) is the logarithmic inverse of the fraction of documents that contain the term. (meaning, divide total number of sentences by the number of sentences containing that word, and then take the log).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0xiJoHUGRcY"
      },
      "outputs": [],
      "source": [
        "# Initialize vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features= 500, # consider only the top 500 common words\n",
        "                             max_df=0.7) # Ignore words that appear in more than 70% of documents\n",
        "#we first apply the vectorizer to each column separately.\n",
        "X0 = vectorizer.fit_transform(df_textual_cleaned.iloc[:,0])\n",
        "X1 = vectorizer.fit_transform(df_textual_cleaned.iloc[:,1])\n",
        "X2 = vectorizer.fit_transform(df_textual_cleaned.iloc[:,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNxiKcveFVas"
      },
      "outputs": [],
      "source": [
        "#then we combine them.\n",
        "X_textual = hstack((X0, X1, X2))\n",
        "X_textual.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO5HR8TUD7V1"
      },
      "outputs": [],
      "source": [
        "y_textual = df_textual_cleaned.iloc[:,-1]\n",
        "y_textual.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtRKhAcLpQ15"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igLemM6sGR8V"
      },
      "outputs": [],
      "source": [
        "X_textual_train, X_textual_val, y_textual_train, y_textual_val = train_test_split(X_textual, y_textual, test_size = 0.20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP-gGaqEpezv"
      },
      "outputs": [],
      "source": [
        "# create a model\n",
        "MNB = MultinomialNB()\n",
        "\n",
        "# fit to data\n",
        "MNB.fit(X_textual_train, y_textual_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4X20tyyHDZk"
      },
      "outputs": [],
      "source": [
        "# Training accuracy\n",
        "y_textual_train_pred = MNB.predict(X_textual_train)\n",
        "accuracy_score(y_textual_train, y_textual_train_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiY9-HZlHevh"
      },
      "outputs": [],
      "source": [
        "# Validation accuracy\n",
        "y_textual_val_pred = MNB.predict(X_textual_val)\n",
        "accuracy_score(y_textual_val, y_textual_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4OWgAvFpQaQ"
      },
      "source": [
        "## Predicting on test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acXi2vvBpGXx"
      },
      "source": [
        "In order to predict data, we must import the given test data and apply the same operations we did on our training data in order for our model to recognize it. In this case, we have to clean and vectorize our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCzgjajHH9Xe"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(\"HousePrice_Test.csv\")\n",
        "df_textual_test = df_test[[\"Description\",\"Services\",\"Features\"]]\n",
        "df_textual_cleaned_test = df_textual_test.iloc[:,:-1].applymap(clean) # Apply cleaning to entire dataset\n",
        "X0_test = vectorizer.fit_transform(df_textual_cleaned.iloc[:,0])\n",
        "X1_test = vectorizer.fit_transform(df_textual_cleaned.iloc[:,1])\n",
        "X2_test = vectorizer.fit_transform(df_textual_cleaned.iloc[:,2])\n",
        "X_textual_test = hstack((X0_test, X1_test, X2_test))\n",
        "y_pred = MNB.predict(X_textual_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRTtFJq3o4uf"
      },
      "source": [
        "For the Kaggle competition, we are required to submit our predictions in a specific format. The following code takes our predictions and creates the file for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV85HHsEGpiE"
      },
      "outputs": [],
      "source": [
        "# save predictions in a file\n",
        "df_id = pd.DataFrame(data=np.arange(1639,2341), columns = ['Index'])\n",
        "df_class = pd.DataFrame(data=y_pred, columns = ['Price'])\n",
        "df_pred = pd.concat([df_id, df_class], axis=1)\n",
        " \n",
        "# change 'YourTeam' by your team number, for instance: Team-1\n",
        "df_pred.to_csv('CS6501_Kaggle_TeamFourteen.csv', sep=',', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYW6ZoFqUpjx"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74jEuvvrpX2C"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-oKlpHEZep1"
      },
      "source": [
        "\"In general, with neural networks, it’s safe to input missing values as 0, with the condition that 0 isn’t already a meaningful value. The network will learn from exposure to the data that the value 0 means missing data and will start ignoring the value.\"\n",
        "\n",
        "Since our data is scaled from 0 to 1, we impute(replace) the missing values with -1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "B6KxKHhFZe9P",
        "outputId": "37f7dd75-1ae8-49dc-9908-23e652fb2721"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7fd2fa859b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_numeric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_numeric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_numeric' is not defined"
          ]
        }
      ],
      "source": [
        "df_numeric = df_numeric.fillna(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8qYN7nOKOw9"
      },
      "outputs": [],
      "source": [
        "X_numeric = df_numeric.iloc[:,:-1]\n",
        "X_numeric.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZtod051LU3W"
      },
      "outputs": [],
      "source": [
        "y_numeric = df_numeric.iloc[:,-1]\n",
        "y_numeric.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bqs_nqWrMfWO"
      },
      "outputs": [],
      "source": [
        "X_numeric_train, X_numeric_val, y_numeric_train, y_numeric_val = train_test_split(X_numeric, y_numeric, test_size = 0.20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS-qKalsqKs1"
      },
      "source": [
        "Since neural networks work with multiple layers, we have to define each layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AShW3mN3J_d7"
      },
      "outputs": [],
      "source": [
        "regressor = Sequential()\n",
        "\n",
        "num_units = 300 # Number of units per layer\n",
        "drop_value = 0.65 # Change to drop out connections\n",
        "\n",
        "# Add LSTM layer\n",
        "regressor.add(LSTM(units = num_units, \n",
        "                   return_sequences = True, \n",
        "                   input_shape = (X_numeric_train.shape[1], 1)))\n",
        "\n",
        "# Add dropout layer\n",
        "regressor.add(Dropout(drop_value))\n",
        "\n",
        "regressor.add(LSTM(units = num_units, return_sequences = True))\n",
        "regressor.add(Dropout(drop_value))\n",
        "\n",
        "regressor.add(LSTM(units = num_units, return_sequences = True))\n",
        "regressor.add(Dropout(drop_value))\n",
        "\n",
        "regressor.add(LSTM(units = num_units))\n",
        "regressor.add(Dropout(drop_value))\n",
        "\n",
        "# Add dense layer for output predictions\n",
        "regressor.add(Dense(units = 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH3Nn6QUqt8t"
      },
      "source": [
        "While comipling we specify an optimization algorithm, a loss metric to minimize, and a performance metric to measure and report. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm_vFtMBqfkX"
      },
      "outputs": [],
      "source": [
        "regressor.compile(optimizer='adam', # Configures the model for training using \"Adam\" optimizer\n",
        "              loss='mean_squared_error', # Loss function\n",
        "              metrics=['accuracy']) # Performance metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU3zQS6Qqurx"
      },
      "source": [
        "In order to make training more efficient, we can define “callbacks”.\n",
        "\n",
        "Following callbacks are implemented:  \n",
        "* ModelCheckpoint – Saves the best model (whichever has the highest metric-accuracy in our case)\n",
        "* EarlyStopping – Stops the model if performance does not improve for a certain number of epochs\n",
        "* ReduceLROnPlateau – Reduces the learning rate if performance stagnates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r_aYaV8PRIl"
      },
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "checkpoint = ModelCheckpoint(\"checkpoints\", # Directory\n",
        "                             monitor='accuracy', # Performance metric to monitor\n",
        "                             verbose=1, # Print update messages\n",
        "                             save_best_only=True, # Save only best performing model\n",
        "                             save_weights_only=False, # Save only weights from model\n",
        "                             mode='max', # Criteria to replace saved model\n",
        "                             save_freq='epoch') # Frequency to save model\n",
        "\n",
        "earlystop = EarlyStopping(monitor='accuracy',\n",
        "                          min_delta=1e-4, # Minimum change in the monitored quantity\n",
        "                          patience=7, # Number of epochs with no improvement\n",
        "                          verbose=1,\n",
        "                          mode='max',\n",
        "                          baseline=None, # Baseline value for the monitored quantity\n",
        "                          restore_best_weights=True) # restore model weights from the epoch with the best value of the monitored quantity\n",
        "\n",
        "lrreduction = ReduceLROnPlateau(monitor='accuracy',\n",
        "                                factor=0.01, # new lr = lr * factor.\n",
        "                                patience = 4, # number of epochs with no improvement\n",
        "                                verbose=1,\n",
        "                                mode='max',\n",
        "                                min_delta=1e-4, # threshold for measuring the new optimum\n",
        "                                cooldown=0, # number of epochs to wait before resuming normal operation after lr has been reduced\n",
        "                                min_lr=1e-6) # lower bound on the learning rate\n",
        "\n",
        "callbacks = [checkpoint, earlystop, lrreduction]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvfQEZP9KDCF"
      },
      "outputs": [],
      "source": [
        "num_epochs = 128\n",
        "regressor.fit(X_numeric_train, y_numeric_train, \n",
        "              epochs = num_epochs, \n",
        "              batch_size = 32,\n",
        "              callbacks = callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiodtGgbsBZ0"
      },
      "source": [
        "We load the \"best\" model defined from our callbacks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPWbtGBfPsxN"
      },
      "outputs": [],
      "source": [
        "best_model = keras.models.load_model('checkpoints')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMW09gfUf24B"
      },
      "outputs": [],
      "source": [
        "# Training set mean squared error (MSE)\n",
        "y_numeric_train_pred = best_model.predict(X_numeric_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg37ziNAgukn"
      },
      "outputs": [],
      "source": [
        "y_numeric_train_pred = scaler.inverse_transform(y_numeric_train_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnHJaqitg_4N"
      },
      "outputs": [],
      "source": [
        "# Validation MSE\n",
        "y_numeric_val_pred = best_model.predict(X_numeric_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnoo5VuPgJa1"
      },
      "outputs": [],
      "source": [
        "y_numeric_val_pred = scaler.inverse_transform(y_numeric_val_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu09e0SXgJXw"
      },
      "outputs": [],
      "source": [
        "mean_squared_error(y_numeric_val, y_numeric_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmlRdLGXpX2C"
      },
      "source": [
        "## Predicting on test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKeuo-CKHYOQ"
      },
      "source": [
        "A key detail to remember is that while the encoder is fit on training data and applied to test data with transform(), the scaler must be fit to training data too. We must also remember to use the scaler to inverse transform the data in order to get non-scaled predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKHN1QbKpdWY"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(\"HousePrice_Test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19XRkBaffP1J"
      },
      "outputs": [],
      "source": [
        "df_numeric_test = df_test[[\"Location\",\"Num_Bathrooms\",\"Num_Beds\",\"BER_class\",\"Latitude\",\"Longitude\",\"Type\",\"Surface\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j_o63R2gSj7"
      },
      "outputs": [],
      "source": [
        "encoded_data = encoder.transform(df_numeric_test[columns_to_replace]) # Encode them\n",
        "encoded_data = pd.DataFrame(encoded_data) # Convert this to a dataframe\n",
        "encoded_data.columns = encoder.get_feature_names_out() # Rename encoded columns\n",
        "\n",
        "for column in columns_to_replace: # Iterate through list of columns\n",
        "    df_numeric_test.drop(column ,axis=1, inplace=True) # Drop (delete) the ones we encoded\n",
        "df_numeric_test = pd.concat([encoded_data,df_numeric_test],axis=1) # Concatenate (join) the dataframes\n",
        "\n",
        "scaler = MinMaxScaler() # Initialize scaler \n",
        "for column in df_numeric_test.columns:\n",
        "    df_numeric_test[column]=pd.DataFrame(scaler.fit_transform(df_numeric_test[column].values.reshape(-1,1))) # Apply scaler\n",
        "X_numeric_test = df_numeric_test\n",
        "y_pred = best_model.predict(X_numeric_test)\n",
        "y_pred = scaler.inverse_transform(y_pred)\n",
        "# save predictions in a file\n",
        "df_id = pd.DataFrame(data=np.arange(1639,2341), columns = ['Index'])\n",
        "df_class = pd.DataFrame(data=y_pred, columns = ['Price'])\n",
        "df_pred = pd.concat([df_id, df_class], axis=1)\n",
        "\n",
        "df_pred.to_csv('CS6501_Kaggle_TeamThree.csv', sep=',', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0GYCpwEM09T"
      },
      "source": [
        "# SUMMARY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXg3VcGXA191"
      },
      "source": [
        "Two techniques were used to successfully estimate house prices: the first, NLP, and the second, LSTM. We used one hot encoding to transform categorical data to continuous data/numerical data. The text in the Description, Services, and Features columns was filtered for stopwords and lemmatized to extract the word's root for NLP, following which the sentences were vectorized and a sparse matrix was created. The numerical and sparse matrices were combined. The information was then used to train various models. The xgboost model has the best accuracy, with an RME value of 13748.106."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCawMmL6qF0T"
      },
      "source": [
        "# REFERENCES\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyNabqeWqPS7"
      },
      "source": [
        "## Raghavan, Shreyas. “Create a Model to Predict House Prices Using Python.” Medium, Towards Data Science, 20 June 2017, https://towardsdatascience.com/create-a-model-to-predict-house-prices-using-python-d34fe8fad88f. \n",
        "\n",
        "## mason, Britney. “House Price Prediction Using LSTM - Arxiv.” House Price Prediction, https://arxiv.org/pdf/1709.08432. \n",
        "\n",
        "## Mir, Mahsa. “House Prices Prediction Using Deep Learning.” Medium, Towards Data Science, 24 July 2020, https://towardsdatascience.com/house-prices-prediction-using-deep-learning-dea265cc3154. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Wg7VCbX77eAA",
        "35dBCwnwi251",
        "xcujSEA739V2",
        "OYeUy3h6Mdjh",
        "zgKSwPJwMcAa",
        "WL91ShB19RPw",
        "dYmCp6dBpLgB",
        "ZtRKhAcLpQ15",
        "u4OWgAvFpQaQ",
        "74jEuvvrpX2C"
      ],
      "name": "Copy of cs6501-teamfourteen.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
